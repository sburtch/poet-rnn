{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "My objevtive for the machine learning project is to create a Recurrent Neural Network (RNN) that can generate (somewhat) legible poetry based on a collection of poems. To do so, I will clean the data and cut up groups of words into sets of inputs for the network to take. The size of the input i.e. the number of words is yet to be determined considering we want to have a large enough input size to gather important features like rhyme schemes and context for the LSTM cell to remember, but we don't want to be too large in our input size that training is too slow. \n",
    "\n",
    "Instead of feeding in actual words, I will encode each word into an identifier, making the unique set of these identifiers as the Y vector for our supervised learning to take place. The vocabulary vector, our Y vector, will be one hot encoded for the word that was actually present, which the LSTM cell will attempt at predicting for each time step. \n",
    "\n",
    "link to data : https://www.kaggle.com/ishnoor/poetry-analysis-with-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "poems = pd.read_csv(\"all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems['length'] = 0\n",
    "for i in range(len(poems)):\n",
    "    poems['length'][i] = len(poems['content'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data by deleting null entries etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poems = poems.sort_values(by='length') #Sort by length of poem\n",
    "poems = poems[14:len(poems)-5] # Delete tails on both sides\n",
    "poems = poems[poems['content'].str.contains('Published')==False]# Eliminate non-poems with 'Published'\n",
    "print(len(poems))\n",
    "poems = poems[poems['content'].str.contains('from Selected Poems')==False]# Eliminate non-poems with 'from Selected Poems'\n",
    "print(len(poems))\n",
    "poems = poems[poems['content'].str.contains('Collected Poems')==False]# Eliminate non-poems with 'from Collected Poems'\n",
    "print(len(poems))\n",
    "#Eliminate where poem is just intro\n",
    "for ind, row in poems.iterrows():\n",
    "    if row['author'] in row['content'].upper() or str(row['poem name']) in row['content'][:40]:\n",
    "        poems = poems.drop([ind])\n",
    "print(len(poems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_poems = len(poems)\n",
    "poem = poems['content'][:num_poems]\n",
    "poem = poem[poems['length'] > 100]\n",
    "poem = poem[poems['length'] < 1000]\n",
    "poem = poem.reset_index(drop=True)\n",
    "X = poem\n",
    "num_poems = len(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocab size and word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ''\n",
    "for i in range(num_poems):\n",
    "    temp += poem[i] + ' '\n",
    "poem = temp\n",
    "\n",
    "import re\n",
    "#poem = re.sub(' +',' ',poem)\n",
    "poem = poem.lower()\n",
    "poem = re.findall(r'[\\w]+|[\\'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]',poem)\n",
    "words = list(set(poem))\n",
    "vocab_size = len(words)\n",
    "#print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = X[i].replace(\"\\r\\n\",\" \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import  Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer( num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.texts_to_sequences(X)\n",
    "text = pad_sequences(text, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwords = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key,value in tokenizer.word_counts.items():\n",
    "    count += 1\n",
    "    print(key,value)\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((maxwords,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        l = line.split()\n",
    "        if l[0] in word_dict:\n",
    "            indx = word_dict[l[0]]\n",
    "            for i in range(50):\n",
    "                embedding_matrix[indx-1][i] = l[i+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, TimeDistributed, Dense, Activation, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "num_steps = 1000\n",
    "hidden_size = 350\n",
    "use_dropout = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(maxwords, hidden_size, input_length=num_steps))\n",
    "    model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    # model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(maxwords)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.layers[0].weight=embedding_matrix\n",
    "    model.layers[0].trainable=False\n",
    "    \n",
    "    noise = Input(shape=(num_steps,))\n",
    "    gen_poem = model(noise)\n",
    "\n",
    "    return Model(noise, gen_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(maxwords, hidden_size, input_length=num_steps))\n",
    "    model.add(LSTM(hidden_size, return_sequences=False))\n",
    "    # model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.layers[0].weight=embedding_matrix\n",
    "    model.layers[0].trainable=False\n",
    "    \n",
    "    i_poem = Input(shape=(num_steps,))\n",
    "    validity = model(i_poem)\n",
    "\n",
    "    return Model(i_poem, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = generator()\n",
    "\n",
    "# The generator takes noise as input and generates poem\n",
    "z = Input(shape=(num_steps,))\n",
    "g_poem = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated poems as input and determines validity\n",
    "validity = discriminator(g_poem)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
