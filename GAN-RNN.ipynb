{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "My objevtive for the machine learning project is to create a Recurrent Neural Network (RNN) that can generate (somewhat) legible poetry based on a collection of poems with a GAN external network to optimize the RNN potential. To do so, I will take a kaggle dataset with 500+ poems written from the reniassance and modern era of poetry to be used as the 'real' data for the discriminator.\n",
    "\n",
    "Tools:\n",
    "Libraries include pandas, numpy, keras.preprocessing for tokenizer (discussed below), GloVe dictionary (below), and keras.layers/models and the like for neural network model architecture and execution. \n",
    "\n",
    "Various hygiene methods are required to standardize each poem into the same length, as well as adding padding for poems less in length. Instead of feeding in actual words, I will encode each word into a tokenizer, so integers are fed into the network rather than words. Further, to better contend with writing legible poetry, I use feature engineering from https://nlp.stanford.edu/projects/glove/ to encode words to higher dimensional space. Words with similar meaning should have a similar vector space, for example. \n",
    "\n",
    "Both the generator and discriminator have RNN structures, including one LSTM layer to predict the next word based on the series of words prior in sequence. The discriminator will take samples from our real poems and full generated poems to try and classify real from fake. \n",
    "\n",
    "\n",
    "link to data : https://www.kaggle.com/ishnoor/poetry-analysis-with-machine-learning\n",
    "\n",
    "References: some hygiene steps came from https://www.kaggle.com/hsankesara/mr-poet and some GAN framework coding came from https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "poems = pd.read_csv(\"all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding character length column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samburtch/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "poems['length'] = 0\n",
    "for i in range(len(poems)):\n",
    "    poems['length'][i] = len(poems['content'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data by deleting null entries and non-poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "536\n",
      "518\n",
      "465\n"
     ]
    }
   ],
   "source": [
    "poems = poems.sort_values(by='length') #Sort by length of poem\n",
    "poems = poems[14:len(poems)-5] # Delete tails on both sides\n",
    "poems = poems[poems['content'].str.contains('Published')==False]# Eliminate non-poems with 'Published'\n",
    "print(len(poems))\n",
    "poems = poems[poems['content'].str.contains('from Selected Poems')==False]# Eliminate non-poems with 'from Selected Poems'\n",
    "print(len(poems))\n",
    "poems = poems[poems['content'].str.contains('Collected Poems')==False]# Eliminate non-poems with 'from Collected Poems'\n",
    "print(len(poems))\n",
    "#Eliminate where poem is just intro\n",
    "for ind, row in poems.iterrows():\n",
    "    if row['author'] in row['content'].upper() or str(row['poem name']) in row['content'][:40]:\n",
    "        poems = poems.drop([ind])\n",
    "print(len(poems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hygiene: only poems between 100 & 1000 in character length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_poems = len(poems)\n",
    "poem = poems['content'][:num_poems]\n",
    "poem = poem[poems['length'] > 100]\n",
    "poem = poem[poems['length'] < 1000]\n",
    "poem = poem.reset_index(drop=True)\n",
    "X = poem\n",
    "num_poems = len(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocab size and word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ''\n",
    "for i in range(num_poems):\n",
    "    temp += poem[i] + ' '\n",
    "poem = temp\n",
    "\n",
    "import re\n",
    "#poem = re.sub(' +',' ',poem)\n",
    "poem = poem.lower()\n",
    "poem = re.findall(r'[\\w]+|[\\'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]',poem)\n",
    "words = list(set(poem))\n",
    "vocab_size = len(words)\n",
    "#print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                                                   349\n",
      "unique                                                  311\n",
      "top       Potuia, potuia\\r\\nWhite grave goddess,\\r\\nPity...\n",
      "freq                                                      3\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fog comes\\r\\non little cat feet.\\r\\n\\r\\nIt sits looking\\r\\nover harbor and city\\r\\non silent haunches\\r\\nand then moves on.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = X[i].replace(\"\\r\\n\",\" \")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Convert words to integers (tokenizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samburtch/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import  Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer( num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.texts_to_sequences(X)\n",
    "maxlen = 0\n",
    "for i in text:\n",
    "    if len(i) > maxlen:\n",
    "        maxlen = len(i)\n",
    "text = pad_sequences(text, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwords = len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map words to 50-dim vector (embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((maxwords+1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        l = line.split()\n",
    "        if l[0] in word_dict:\n",
    "            indx = word_dict[l[0]]\n",
    "            for i in range(50):\n",
    "                embedding_matrix[indx][i] = l[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 349 x 177 x 50 X_train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((349,177,50),dtype='float32')\n",
    "for indp, poem in enumerate(text):\n",
    "    for indw, word in enumerate(poem):\n",
    "        x_train[indp,indw,:] = embedding_matrix[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, TimeDistributed, Dense, Activation, Input\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "num_steps = 177\n",
    "#hidden_size = 350\n",
    "feature_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = Input(shape=(num_steps,1))\n",
    "\n",
    "x = LSTM(feature_dim, return_sequences=True)(generator_input)\n",
    "#x = TimeDistributed(Dense(feature_dim))(x)\n",
    "\n",
    "generator = Model(generator_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = Input(shape=(num_steps,feature_dim))\n",
    "\n",
    "x = LSTM(feature_dim)(discriminator_input) \n",
    "#x = LSTM(hidden_size, return_state=True)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = Model(discriminator_input, x)\n",
    "\n",
    "discriminator_optimizer = RMSprop(lr=0.0008, clipvalue=1.0) #decay=1e-8\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "\n",
    "# Set discriminator weights to non-trainable\n",
    "# (will only apply to the `gan` model)\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(num_steps,1))\n",
    "gen_output = generator(gan_input)\n",
    "gan_output = discriminator(gen_output)\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = RMSprop(lr=0.0004, clipvalue=1.0) #decay=1e-8\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samburtch/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss at step 0: 0.692512\n",
      "generator loss at step 0: 0.705456\n",
      "discriminator loss at step 10: 0.69128674\n",
      "generator loss at step 10: 0.7360985\n",
      "discriminator loss at step 20: 0.69160616\n",
      "generator loss at step 20: 0.74496174\n",
      "discriminator loss at step 30: 0.68997955\n",
      "generator loss at step 30: 0.7435515\n",
      "discriminator loss at step 40: 0.68980426\n",
      "generator loss at step 40: 0.7476873\n",
      "discriminator loss at step 50: 0.67291605\n",
      "generator loss at step 50: 0.83934313\n",
      "discriminator loss at step 60: 0.106737964\n",
      "generator loss at step 60: 2.5219164\n",
      "discriminator loss at step 70: 0.042421106\n",
      "generator loss at step 70: 3.4494157\n",
      "discriminator loss at step 80: 0.015179757\n",
      "generator loss at step 80: 4.2981086\n",
      "discriminator loss at step 90: 0.005743395\n",
      "generator loss at step 90: 5.1372514\n",
      "discriminator loss at step 100: -0.017270032\n",
      "generator loss at step 100: 6.065909\n",
      "discriminator loss at step 110: -0.031956997\n",
      "generator loss at step 110: 7.2712293\n",
      "discriminator loss at step 120: -0.032872453\n",
      "generator loss at step 120: 8.284627\n",
      "discriminator loss at step 130: -0.059759676\n",
      "generator loss at step 130: 9.220318\n",
      "discriminator loss at step 140: -0.08629398\n",
      "generator loss at step 140: 10.13158\n",
      "discriminator loss at step 150: -0.073051766\n",
      "generator loss at step 150: 10.885447\n",
      "discriminator loss at step 160: -0.0728641\n",
      "generator loss at step 160: 11.581528\n",
      "discriminator loss at step 170: -0.11473284\n",
      "generator loss at step 170: 12.219996\n",
      "discriminator loss at step 180: -0.064592525\n",
      "generator loss at step 180: 12.686953\n",
      "discriminator loss at step 190: -0.11313758\n",
      "generator loss at step 190: 13.097606\n",
      "discriminator loss at step 200: -0.12689926\n",
      "generator loss at step 200: 13.715704\n",
      "discriminator loss at step 210: -0.10717399\n",
      "generator loss at step 210: 14.27028\n",
      "discriminator loss at step 220: -0.12318541\n",
      "generator loss at step 220: 14.767058\n",
      "discriminator loss at step 230: -0.13554716\n",
      "generator loss at step 230: 15.249238\n",
      "discriminator loss at step 240: -0.12618078\n",
      "generator loss at step 240: 15.91928\n",
      "discriminator loss at step 250: -0.17105427\n",
      "generator loss at step 250: 15.412425\n",
      "discriminator loss at step 260: -0.08488265\n",
      "generator loss at step 260: 15.942385\n",
      "discriminator loss at step 270: -0.10605526\n",
      "generator loss at step 270: 15.413759\n",
      "discriminator loss at step 280: -0.1534472\n",
      "generator loss at step 280: 15.942385\n",
      "discriminator loss at step 290: -0.11273985\n",
      "generator loss at step 290: 15.411542\n",
      "discriminator loss at step 300: -0.11829889\n",
      "generator loss at step 300: 15.942385\n",
      "discriminator loss at step 310: 0.051014945\n",
      "generator loss at step 310: 15.414158\n",
      "discriminator loss at step 320: -0.071683265\n",
      "generator loss at step 320: 15.413167\n",
      "discriminator loss at step 330: -0.024591008\n",
      "generator loss at step 330: 15.412448\n",
      "discriminator loss at step 340: -0.14477251\n",
      "generator loss at step 340: 15.942385\n",
      "discriminator loss at step 350: -0.014179422\n",
      "generator loss at step 350: 13.294113\n",
      "discriminator loss at step 360: -0.15393963\n",
      "generator loss at step 360: 15.942385\n",
      "discriminator loss at step 370: -0.12666719\n",
      "generator loss at step 370: 15.942385\n",
      "discriminator loss at step 380: -0.186239\n",
      "generator loss at step 380: 15.942385\n",
      "discriminator loss at step 390: -0.1306999\n",
      "generator loss at step 390: 15.942385\n",
      "discriminator loss at step 400: -0.15788367\n",
      "generator loss at step 400: 15.942385\n",
      "discriminator loss at step 410: -0.15476899\n",
      "generator loss at step 410: 15.942385\n",
      "discriminator loss at step 420: -0.1420035\n",
      "generator loss at step 420: 15.942385\n",
      "discriminator loss at step 430: -0.12205355\n",
      "generator loss at step 430: 15.942385\n",
      "discriminator loss at step 440: -0.13736342\n",
      "generator loss at step 440: 15.942385\n",
      "discriminator loss at step 450: -0.16370569\n",
      "generator loss at step 450: 15.942385\n",
      "discriminator loss at step 460: -0.15411207\n",
      "generator loss at step 460: 15.942385\n",
      "discriminator loss at step 470: -0.104836464\n",
      "generator loss at step 470: 15.942385\n",
      "discriminator loss at step 480: -0.11434229\n",
      "generator loss at step 480: 15.942385\n",
      "discriminator loss at step 490: -0.12101927\n",
      "generator loss at step 490: 15.942385\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "batch_size = 30\n",
    "\n",
    "# Start training loop\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,num_steps, 1))\n",
    "\n",
    "    # Decode them to fake poems\n",
    "    generated_poems = generator.predict(random_latent_vectors)\n",
    "\n",
    "    # Combine them with real poems\n",
    "    stop = start + batch_size\n",
    "    real_poems = x_train[start: stop]\n",
    "    combined_poems = np.concatenate([generated_poems, real_poems])\n",
    "\n",
    "    # Assemble labels discriminating real from fake poems\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    # Add random noise to the labels - important trick!\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_poems, labels)\n",
    "\n",
    "    # sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, num_steps, 1))\n",
    "\n",
    "    # Assemble labels that say \"all real poems\"\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the generator (via the gan model,\n",
    "    # where the discriminator weights are frozen)\n",
    "    g_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        # Save model weights\n",
    "        #gan.save_weights('gan.h5')\n",
    "\n",
    "        # Print metrics\n",
    "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
    "        print('generator loss at step %s: %s' % (step, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
